{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST Without keras.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtZeLUnPDJSE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0ee71513-689c-430b-cd9d-2e4167cf52ba"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import mnist\n",
        "\n",
        "(x_train,y_train),(x_test,y_test) = mnist.load_data()\n",
        "#preprocessing (normalization)\n",
        "\n",
        "x_train = x_train.reshape(60000,784)/255\n",
        "x_test = x_test.reshape(10000,784)/255\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 784)\n",
            "(10000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SavyaurDmco",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#input layer = 784 neurons\n",
        "#hidden layer = 15 neurons\n",
        "#output layer = 10 neurons\n",
        "\n",
        "#bias\n",
        "bias_input_to_hidden, reverse_bias_input_to_hidden   = [0] * 15 , [0] * 15\n",
        "bias_hidden_to_output, reverse_bias_hidden_to_output = [0] * 10 , [0] * 10\n",
        "\n",
        "#weight\n",
        "weight_input_to_hidden, reverse_weight_input_to_hidden = [[0 for i in range(784)]for i in range(15)] , [[0 for i in range(784)]for i in range(15)]\n",
        "weight_hidden_to_output, reverse_weight_hidden_to_output = [[0 for i in range(15)]for i in range(10)] , [[0 for i in range(15)]for i in range(10)]\n",
        "\n",
        "#input , output\n",
        "output_from_middle_layer, input_to_middle_layer = [0] * 15 , [0] * 15\n",
        "output_from_output_layer , input_to_output_layer = [0] * 10 , [0] * 10\n",
        "\n",
        "#test input , test output\n",
        "test_output_from_middle_layer, test_input_to_middle_layer = [0] * 15 , [0] * 15\n",
        "test_output_from_output_layer, test_input_to_output_layer = [0] * 10 , [0] * 10"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT4mCkoYHKBv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#adding random numbers to bias and weights\n",
        "\n",
        "for i in range(15):\n",
        "  bias_input_to_hidden[i] = np.random.rand() * 0.1\n",
        "for i in range(10):\n",
        "  bias_hidden_to_output[i] = np.random.rand() * 0.1\n",
        "\n",
        "for i in range(15):\n",
        "  for j in range(784):\n",
        "    weight_input_to_hidden[i][j] = np.random.randn()*0.1\n",
        "\n",
        "for i in range(10):\n",
        "  for j in range(15):\n",
        "    weight_hidden_to_output[i][j] = np.random.randn()*0.1"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wCH9SXBIIdG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#defining activation functions -> sigmoid , softmax\n",
        "\n",
        "def sigmoid (x):\n",
        "  exponent = 1 + np.exp(-x)\n",
        "  return (1/exponent)\n",
        "\n",
        "def diff_sigmoid(x):\n",
        "  sig = sigmoid(x)\n",
        "  return (1-sig) * sig\n",
        "\n",
        "#softmax is used at output layer\n",
        "def softmax(output_array):\n",
        "  maxi = np.max(output_array) #finding maximum\n",
        "  _exp = np.exp(output_array - maxi) #finding exponents by reducing maximum from the original array\n",
        "  sum_exp = np.sum(_exp)\n",
        "  answer_array = _exp / sum_exp #dividing exponent with sum of array\n",
        "  return answer_array\n",
        "\n",
        "#actual and expected are one hot encodings\n",
        "def delta(actual_number , expected_number , output_from_output_layer, input_to_output_layer, weight_hidden_to_output):\n",
        "  _sum = 0\n",
        "\n",
        "  for i in range(10):\n",
        "    op_diff = output_from_output_layer[i] - expected_number[i]\n",
        "    weight = weight_hidden_to_output[i][actual_number]\n",
        "    diff_sig = diff_sigmoid(input_to_output_layer[i])\n",
        "\n",
        "    _sum += op_diff * weight * diff_sig\n",
        "\n",
        "  return _sum"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGRRYtBaM7Rf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def back_propogation(output_from_middle_layer, output_from_output_layer, input_to_middle_layer, input_to_output_layer, expected_number, x_train, learning_rate):\n",
        "  #getting global bias and weights\n",
        "\n",
        "  global weight_input_to_hidden\n",
        "  global weight_hidden_to_output\n",
        "  global bias_input_to_hidden\n",
        "  global bias_hidden_to_output\n",
        "\n",
        "  #changing weight -> going from output to hidden\n",
        "\n",
        "  for i in range(10):\n",
        "    _diff = output_from_output_layer[i] - expected_number[i]\n",
        "    _sig_diff = diff_sigmoid(input_to_output_layer[i])\n",
        "\n",
        "    for j in range(15):\n",
        "      reverse_weight_hidden_to_output[i][j]  = _diff * _sig_diff\n",
        "      weight_hidden_to_output[i][j] -= learning_rate * reverse_weight_hidden_to_output[i][j] * output_from_middle_layer[j]\n",
        "  \n",
        "  #changing weight -> going from hidden to input\n",
        "\n",
        "  for i in range(15):\n",
        "    _delta = delta(i, expected_number, output_from_output_layer, input_to_output_layer, weight_hidden_to_output)\n",
        "    _sig_diff = diff_sigmoid(input_to_middle_layer[i])\n",
        "    for j in range(784):\n",
        "      reverse_weight_input_to_hidden[i][j] = _delta * _sig_diff\n",
        "      weight_input_to_hidden[i][j] -= learning_rate * reverse_weight_input_to_hidden[i][j] * x_train[j]\n",
        "  \n",
        "  #changing bias -> going from output to hidden\n",
        "\n",
        "  for i in range(10):\n",
        "    _diff = output_from_output_layer[i] - expected_number[i]\n",
        "    _sig_diff = diff_sigmoid(input_to_output_layer[i])\n",
        "\n",
        "    reverse_bias_hidden_to_output[i] = _diff * _sig_diff\n",
        "    bias_hidden_to_output[i] -= learning_rate * reverse_bias_hidden_to_output[i]\n",
        "\n",
        "  #changing bias -> going from hidden to input\n",
        "\n",
        "  for i in range(15):\n",
        "    _delta = delta(i, expected_number, output_from_output_layer, input_to_output_layer, weight_hidden_to_output)\n",
        "    _sig_diff = diff_sigmoid(input_to_middle_layer[i])\n",
        "\n",
        "    reverse_bias_input_to_hidden[i] = _delta * _sig_diff\n",
        "    bias_input_to_hidden[i] -= learning_rate * reverse_bias_input_to_hidden[i]\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TZmV-NGVA0q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#defining accuracy and error calculation functions\n",
        "\n",
        "def accuracy(predicted_answer, actual_answer , switch):\n",
        "  max_pred = np.argmax(predicted_answer, axis=1)\n",
        "  max_train = np.argmax(actual_answer, axis=1)\n",
        "\n",
        "  if switch == \"train\":\n",
        "    return np.sum(max_pred == max_train) / 100 #batch size = 100\n",
        "  elif switch == \"test\":\n",
        "    return np.sum(max_pred == max_train) / 10000\n",
        "\n",
        "def sum_of_squares_error(output_from_output_layer,expected_number):\n",
        "  square = output_from_output_layer - expected_number\n",
        "  return 0.5 * np.sum(square**2)\n",
        "\n",
        "def make_round_num(n):\n",
        "  rounds = [0] * n\n",
        "  for i in range(n):\n",
        "    rounds[i] = i\n",
        "  \n",
        "  return rounds\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgckOsK8ZnA0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.3\n",
        "epochs = 5\n",
        "input_words = 5 #rounds  within epoch"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4USwR6QZu7x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dd5e0e25-913d-4fec-9b89-d07da7be8e66"
      },
      "source": [
        "all_train_accuracy = []\n",
        "all_train_loss = []\n",
        "\n",
        "for l in range(epochs):\n",
        "    print(\"<---------------epoch no : \"+str(l)+\"---------------->\")\n",
        "\n",
        "    for k in range(input_words):\n",
        "\n",
        "        train_prediction = []\n",
        "        train_answer = []\n",
        "\n",
        "        print(\"Round number : \"+str(l*input_words+k))\n",
        "\n",
        "        for j in range(100):\n",
        "          #input to hidden \n",
        "          for i in range(15):\n",
        "\n",
        "            input_to_middle_layer[i] = np.dot(x_train[k*100+j], weight_input_to_hidden[i]) + bias_input_to_hidden[i]\n",
        "            output_from_middle_layer[i] = sigmoid(input_to_middle_layer[i])\n",
        "            \n",
        "\n",
        "\n",
        "          #hidden to output\n",
        "\n",
        "          for i in range(10):\n",
        "            input_to_output_layer[i] = np.dot(output_from_middle_layer, weight_hidden_to_output[i]) + bias_hidden_to_output[i]\n",
        "\n",
        "\n",
        "          #softmaxing\n",
        "          \n",
        "          output_from_output_layer = softmax(input_to_output_layer)\n",
        "\n",
        "          expected_num = [0]*10 #one hot encoding\n",
        "          expected_num[y_train[k*100+j]] = expected_num[y_train[k*100+j]] + 1\n",
        "\n",
        "          train_prediction.append(output_from_output_layer)\n",
        "          train_answer.append(expected_num)\n",
        "          back_propogation(output_from_middle_layer,output_from_output_layer, input_to_middle_layer, input_to_output_layer, expected_num, x_train[k*100+j], learning_rate)\n",
        "\n",
        "        train_acc = accuracy(train_prediction, train_answer, \"train\")\n",
        "        train_loss = sum_of_squares_error(output_from_output_layer, expected_num)\n",
        "\n",
        "        print(\"train_accuracy = \"+str(train_acc))\n",
        "        print(\"train_loss     = \"+str(train_loss))\n",
        "            \n",
        "        all_train_accuracy.append(train_acc)\n",
        "        all_train_loss.append(train_loss)\n",
        "\n",
        "\n",
        "\n",
        "          \n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<---------------epoch no : 0---------------->\n",
            "Round number : 0\n",
            "train_accuracy = 0.62\n",
            "train_loss     = 0.22462521747506714\n",
            "Round number : 1\n",
            "train_accuracy = 0.65\n",
            "train_loss     = 0.29335020260807837\n",
            "Round number : 2\n",
            "train_accuracy = 0.67\n",
            "train_loss     = 0.10042751658328568\n",
            "Round number : 3\n",
            "train_accuracy = 0.72\n",
            "train_loss     = 0.25251673693595605\n",
            "Round number : 4\n",
            "train_accuracy = 0.67\n",
            "train_loss     = 0.3430819701192154\n",
            "<---------------epoch no : 1---------------->\n",
            "Round number : 5\n",
            "train_accuracy = 0.77\n",
            "train_loss     = 0.04702156243272173\n",
            "Round number : 6\n",
            "train_accuracy = 0.82\n",
            "train_loss     = 0.14324323902644487\n",
            "Round number : 7\n",
            "train_accuracy = 0.82\n",
            "train_loss     = 0.025274022436063772\n",
            "Round number : 8\n",
            "train_accuracy = 0.81\n",
            "train_loss     = 0.1310414648654346\n",
            "Round number : 9\n",
            "train_accuracy = 0.83\n",
            "train_loss     = 0.1448309016868208\n",
            "<---------------epoch no : 2---------------->\n",
            "Round number : 10\n",
            "train_accuracy = 0.91\n",
            "train_loss     = 0.019430675003945196\n",
            "Round number : 11\n",
            "train_accuracy = 0.89\n",
            "train_loss     = 0.05134570419728288\n",
            "Round number : 12\n",
            "train_accuracy = 0.9\n",
            "train_loss     = 0.012965991655083057\n",
            "Round number : 13\n",
            "train_accuracy = 0.92\n",
            "train_loss     = 0.13532625208054305\n",
            "Round number : 14\n",
            "train_accuracy = 0.9\n",
            "train_loss     = 0.04475246420927637\n",
            "<---------------epoch no : 3---------------->\n",
            "Round number : 15\n",
            "train_accuracy = 0.94\n",
            "train_loss     = 0.012386591067775945\n",
            "Round number : 16\n",
            "train_accuracy = 0.93\n",
            "train_loss     = 0.021440992583344014\n",
            "Round number : 17\n",
            "train_accuracy = 0.94\n",
            "train_loss     = 0.00836034553909406\n",
            "Round number : 18\n",
            "train_accuracy = 0.96\n",
            "train_loss     = 0.1061703651918118\n",
            "Round number : 19\n",
            "train_accuracy = 0.94\n",
            "train_loss     = 0.01176308952449169\n",
            "<---------------epoch no : 4---------------->\n",
            "Round number : 20\n",
            "train_accuracy = 0.97\n",
            "train_loss     = 0.010236558026151647\n",
            "Round number : 21\n",
            "train_accuracy = 0.97\n",
            "train_loss     = 0.00842483547796478\n",
            "Round number : 22\n",
            "train_accuracy = 0.94\n",
            "train_loss     = 0.0058881628548367055\n",
            "Round number : 23\n",
            "train_accuracy = 0.97\n",
            "train_loss     = 0.06221325220009676\n",
            "Round number : 24\n",
            "train_accuracy = 0.98\n",
            "train_loss     = 0.004539262441081348\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwJGm_Rgp1cK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b8567029-1c67-40e2-fb4d-60cd9f837bf7"
      },
      "source": [
        "test_prediction = []\n",
        "test_answer = []\n",
        "\n",
        "for j in range(10000):\n",
        "\n",
        "  for i in range(15):\n",
        "    test_input_to_middle_layer[i] = np.dot(x_test[j], weight_input_to_hidden[i]) + bias_input_to_hidden[i]\n",
        "    test_output_from_middle_layer[i] = sigmoid(test_input_to_middle_layer[i])\n",
        "\n",
        "  for i in range(10):\n",
        "    test_input_to_output_layer[i] = np.dot(test_output_from_middle_layer , weight_hidden_to_output[i]) + bias_hidden_to_output[i]\n",
        "\n",
        "  test_output_from_output_layer = softmax(test_input_to_output_layer)\n",
        "  expected_number = [0] * 10\n",
        "  expected_number[y_test[j]]= expected_number[y_test[j]]+1\n",
        "\n",
        "  test_prediction.append(test_output_from_output_layer)\n",
        "  test_answer.append(expected_number)\n",
        "\n",
        "test_acc = accuracy(test_prediction, test_answer, \"test\")\n",
        "test_loss = sum_of_squares_error(test_output_from_output_layer , expected_number)\n",
        "\n",
        "print(\"test_accuracy = \"+str(test_acc))\n",
        "print(\"test_loss     = \"+str(test_loss))\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_accuracy = 0.8213\n",
            "test_loss     = 0.0031564438795340334\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}